\chapter{Graph}

\section{Network flow}
	\kactlimport{MinCostMaxFlow.h}
	\kactlimport{Dinic.h}
	\kactlimport{GlobalMinCut.h}
	\kactlimport{GomoryHu.h}

\section{Matching}
	\kactlimport{hopcroftKarp.h}
	\kactlimport{MinimumVertexCover.h}
	\kactlimport{WeightedMatching.h}
	\kactlimport{GeneralMatching.h}

\section{DFS algorithms}
	\kactlimport{BiconnectedComponents.h}
	\kactlimport{2sat.h}
	\kactlimport{EulerWalk.h}

\section{Trees}
	\kactlimport{HLD.h}
	\kactlimport{CentroidTree.h}
	\kactlimport{LinkCutTree.h}
	\kactlimport{DirectedMST.h}
	\kactlimport{DominatorTree.h}
	
\section{Various}
	\kactlimport{kShortestWalks.h}
	\kactlimport{SubgraphsCounting.h}
	\kactlimport{EdgeColoring.h}
	\kactlimport{MaximumClique.h}
	\kactlimport{MaximalCliques.h}
	\kactlimport{GreenHackenbush.h}
	\kactlimport{RedBlueHackenbush.h}

\section{Math}
	\subsection{Number of Spanning Trees}
		% I.e. matrix-tree theorem.
		% Source: https://en.wikipedia.org/wiki/Kirchhoff%27s_theorem
		% Test: stress-tests/graph/matrix-tree.cpp
		Create an $N\times N$ matrix \texttt{mat}, and for each edge $a \rightarrow b \in G$, do
		\texttt{mat[a][b]--, mat[b][b]++} (and \texttt{mat[b][a]--, mat[a][a]++} if $G$ is undirected).
		Remove the $i$th row and column and take the determinant; this yields the number of directed spanning trees rooted at $i$
		(if $G$ is undirected, remove any row/column).

	\subsection{Erdős–Gallai theorem}
		% Source: https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem
		% Test: stress-tests/graph/erdos-gallai.cpp
		A simple graph with node degrees $d_1 \ge \dots \ge d_n$ exists iff $d_1 + \dots + d_n$ is even and for every $k = 1\dots n$,
		\[ \sum _{i=1}^{k}d_{i}\leq k(k-1)+\sum _{i=k+1}^{n}\min(d_{i},k). \]

	\subsection{Markov chains}
		A \emph{Markov chain} is a discrete random process with the property that the next state depends only on the current state.
		Let $X_1,X_2,\ldots$ be a sequence of random variables generated by the Markov process.
		Then there is a transition matrix $\mathbf{P} = (p_{ij})$, with $p_{ij} = \Pr(X_n = i | X_{n-1} = j)$,
		and $\mathbf{p}^{(n)} = \mathbf P^n \mathbf p^{(0)}$ is the probability distribution for $X_n$ (i.e., $p^{(n)}_i = \Pr(X_n = i)$),
		where $\mathbf{p}^{(0)}$ is the initial distribution.

		$\mathbf{\pi}$ is a stationary distribution if $\mathbf{\pi} = \mathbf{\pi P}$.
		If the Markov chain is \emph{irreducible} (it is possible to get to any state from any state),
		then $\pi_i = \frac{1}{\mathbb{E}(T_i)}$ where $\mathbb{E}(T_i)$  is the expected time between two visits in state $i$.
		$\pi_j/\pi_i$ is the expected number of visits in state $j$ between two visits in state $i$.

		For a connected, undirected and non-bipartite graph, where the transition probability is uniform among all neighbors, $\pi_i$ is proportional to node $i$'s degree.

		A Markov chain is \emph{ergodic} if the asymptotic distribution is independent of the initial distribution.
		A finite Markov chain is ergodic iff it is irreducible and \emph{aperiodic} (i.e., the gcd of cycle lengths is 1).
		$\lim_{k\rightarrow\infty}\mathbf{P}^k = \mathbf{1}\pi$.

		A Markov chain is an A-chain if the states can be partitioned into two sets $\mathbf{A}$ and $\mathbf{G}$, such that all states in $\mathbf{A}$ are absorbing ($p_{ii}=1$), and all states in $\mathbf{G}$ leads to an absorbing state in $\mathbf{A}$.
		The probability for absorption in state $i\in\mathbf{A}$, when the initial state is $j$, is $a_{ij} = p_{ij}+\sum_{k\in\mathbf{G}} a_{ik}p_{kj}$.
		The expected time until absorption, when the initial state is $i$, is $t_i = 1+\sum_{k\in\mathbf{G}}p_{ki}t_k$.
